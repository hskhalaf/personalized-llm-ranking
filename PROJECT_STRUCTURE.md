# Project Structure

This document outlines the complete structure of the Personalized LLM Ranking Experiment project.

## ğŸ“ Directory Structure

```
personalized-llm-ranking/
â”œâ”€â”€ ğŸ“‹ Core Configuration
â”‚   â”œâ”€â”€ config.py                 # Central configuration and parameters
â”‚   â”œâ”€â”€ requirements.txt          # Python dependencies
â”‚   â””â”€â”€ .gitignore               # Git ignore patterns
â”‚
â”œâ”€â”€ ğŸ”§ Core Modules
â”‚   â”œâ”€â”€ data_generation.py       # Synthetic data generation
â”‚   â”œâ”€â”€ elo_system.py            # Elo Offset system implementation
â”‚   â”œâ”€â”€ reward_model.py          # Fine-tuned reward model system
â”‚   â”œâ”€â”€ evaluation.py            # Metrics and evaluation
â”‚   â””â”€â”€ main.py                  # Main experiment orchestration
â”‚
â”œâ”€â”€ ğŸ“ Prompt Templates
â”‚   â”œâ”€â”€ prompts/
â”‚   â”‚   â”œâ”€â”€ prompt_generation.txt      # Template for generating prompts
â”‚   â”‚   â”œâ”€â”€ preference_evaluation.txt  # Template for pairwise preferences
â”‚   â”‚   â””â”€â”€ ground_truth_ranking.txt   # Template for ground truth rankings
â”‚
â”œâ”€â”€ ğŸ’¾ Data & Results (Generated)
â”‚   â”œâ”€â”€ cache/                    # Cached data and intermediate results
â”‚   â”‚   â”œâ”€â”€ arena_elo_scores.json
â”‚   â”‚   â”œâ”€â”€ generated_prompts_{persona}.json
â”‚   â”‚   â”œâ”€â”€ model_responses_{persona}.json
â”‚   â”‚   â”œâ”€â”€ preference_data_{persona}.json
â”‚   â”‚   â”œâ”€â”€ reward_model_{persona}.pt
â”‚   â”‚   â””â”€â”€ elo_state_{persona}.json
â”‚   â””â”€â”€ results/                  # Final results and visualizations
â”‚       â”œâ”€â”€ learning_curves_{persona}.png
â”‚       â”œâ”€â”€ metric_comparison_{persona}.png
â”‚       â””â”€â”€ evaluation_results_{persona}.json
â”‚
â”œâ”€â”€ ğŸ› ï¸ Setup & Utilities
â”‚   â”œâ”€â”€ setup_project.py         # Project setup and validation
â”‚   â”œâ”€â”€ quick_start.sh           # Quick start script
â”‚   â””â”€â”€ PROJECT_STRUCTURE.md     # This file
â”‚
â””â”€â”€ ğŸ“š Documentation
    â””â”€â”€ README.md                 # Main documentation
```

## ğŸ¯ Core Components

### Configuration Layer
- **`config.py`**: Central hub for all configurations
  - Model definitions and Arena ELO mappings
  - Persona definitions (Coder, Academic, Creative Writer)
  - Experiment parameters
  - Path management and caching logic

### Data Processing Layer
- **`data_generation.py`**: Handles all synthetic data generation
  - Prompt generation using LLMs
  - Model response collection
  - Pairwise preference evaluation
  - Ground truth ranking generation
  - Intelligent caching system

### Ranking Systems Layer
- **`elo_system.py`**: Elo Offset system
  - Modifies global ELO scores with user-specific offsets
  - Standard Elo update rules
  - State management and persistence

- **`reward_model.py`**: Personalized Reward Model system
  - Fine-tunes pre-trained Skywork reward model
  - Freezes base model, trains only MLP head
  - Implements rank regularization loss
  - PyTorch-based training pipeline

### Evaluation Layer
- **`evaluation.py`**: Comprehensive evaluation framework
  - Multiple ranking metrics (Kendall's Tau, Spearman's Rho, etc.)
  - Statistical analysis
  - Visualization generation
  - Results comparison

### Orchestration Layer
- **`main.py`**: Main experiment coordinator
  - End-to-end experiment execution
  - Command-line interface
  - Progress monitoring
  - Results aggregation

## ğŸ”„ Data Flow

```
1. Configuration Loading (config.py)
   â†“
2. Prompt Generation (data_generation.py)
   â†“
3. Model Response Collection (data_generation.py)
   â†“
4. Preference Evaluation (data_generation.py)
   â†“
5. System Training
   â”œâ”€â”€ Elo System Updates (elo_system.py)
   â””â”€â”€ Reward Model Training (reward_model.py)
   â†“
6. Evaluation (evaluation.py)
   â†“
7. Results & Visualization (evaluation.py)
```

## ğŸ¨ Prompt Templates

The project uses external prompt templates for better maintainability:

- **`prompt_generation.txt`**: Generates persona-specific prompts
- **`preference_evaluation.txt`**: Evaluates pairwise preferences
- **`ground_truth_ranking.txt`**: Creates ground truth rankings

## ğŸ’¾ Caching Strategy

The project implements intelligent caching:

- **Config-based invalidation**: Cache invalidated when configuration changes
- **Persona-specific**: Separate caches for each persona
- **Component-level**: Individual caches for prompts, responses, preferences
- **Incremental**: Supports both full and incremental updates

## ğŸš€ Usage Patterns

### Basic Usage
```bash
python main.py --persona coder
```

### Advanced Usage
```bash
python main.py --persona academic --incremental --lambda_reg 0.2 --num_prompts 30
```

### Development Setup
```bash
./quick_start.sh  # Complete setup
python setup_project.py  # Validation only
```

## ğŸ”§ Extension Points

The modular design enables easy extensions:

1. **New Personas**: Add to `PERSONAS` in `config.py`
2. **New Models**: Add to `DEFAULT_MODELS` in `config.py`
3. **New Metrics**: Extend `Evaluator` class in `evaluation.py`
4. **New Systems**: Implement similar to `EloOffsetSystem`
5. **New Prompts**: Add templates in `prompts/` directory

## ğŸ“Š Output Artifacts

Each experiment run produces:

- **Cached Data**: Reusable intermediate results
- **Model Checkpoints**: Trained reward model states
- **Evaluation Metrics**: Comprehensive performance analysis
- **Visualizations**: Learning curves and comparison plots
- **Logs**: Detailed execution logs

This structure ensures reproducibility, maintainability, and extensibility for research purposes.